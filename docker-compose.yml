services:
  elasticsearch:
    image: docker.elastic.co/elasticsearch/elasticsearch:8.13.4
    environment:
      - discovery.type=single-node
      - xpack.security.enabled=false
      - ES_JAVA_OPTS=-Xms1g -Xmx1g
    volumes:
      - es_data:/usr/share/elasticsearch/data
    ports:
      - "${ELASTICSEARCH_PORT:-9200}:9200"
    healthcheck:
      test: ["CMD-SHELL", "curl -s http://localhost:9200 >/dev/null || exit 1"]
      interval: 10s
      timeout: 5s
      retries: 10
      start_period: 30s

  kibana:
    image: docker.elastic.co/kibana/kibana:8.13.4
    environment:
      - ELASTICSEARCH_HOSTS=http://elasticsearch:9200
    depends_on:
      elasticsearch:
        condition: service_healthy
    ports:
      - "${KIBANA_PORT:-5601}:5601"

  api:
    build:
      context: .
      dockerfile: Dockerfile.production
    container_name: prostaff-scraper-api
    env_file:
      - .env
    volumes:
      - .:/app
    ports:
      - "8000:8000"
    depends_on:
      elasticsearch:
        condition: service_healthy
    command: ["uvicorn", "api.main:app", "--host", "0.0.0.0", "--port", "8000", "--reload"]
    healthcheck:
      test: ["CMD-SHELL", "curl -f http://localhost:8000/health || exit 1"]
      interval: 15s
      timeout: 5s
      retries: 5
      start_period: 20s

  # Enrichment daemon - populates items/runes/KDA via Leaguepedia + Riot API
  # Runs slowly (1 request per 9s to Leaguepedia) but that's fine for background work.
  # Disable locally if you don't need enrichment: comment out this service.
  enrichment:
    build:
      context: .
      dockerfile: Dockerfile.production
    env_file:
      - .env
    volumes:
      - .:/app
    depends_on:
      elasticsearch:
        condition: service_healthy
    # Check every 30min for unenriched games, process up to 50 per batch
    command: ["python", "etl/enrichment_pipeline.py", "--daemon", "--interval", "30", "--batch", "50"]
    restart: unless-stopped

  # Competitive cron - polls LoL Esports API and indexes new games to Elasticsearch.
  # Runs every SYNC_INTERVAL_HOURS hours (default 1h).
  scraper-cron:
    build:
      context: .
      dockerfile: Dockerfile.production
    env_file:
      - .env
    volumes:
      - .:/app
    depends_on:
      elasticsearch:
        condition: service_healthy
    command:
      - "python"
      - "etl/competitive_pipeline.py"
      - "--daemon"
      - "--production"
      - "--interval"
      - "${SYNC_INTERVAL_HOURS:-1}"
      - "--leagues"
      - "${SYNC_LEAGUES:-CBLOL}"
      - "--limit"
      - "${SYNC_LIMIT:-100}"
    restart: unless-stopped

volumes:
  es_data:
